import numpy as np
from typing import Optional, Dict, Callable, Tuple
import tensorflow as tf
import tensorflow.keras as keras
from utils.model_params import NBeatsParams, BlockType
from utils.train_params import TrainParams


class NBeatsTF(keras.Model):

    def __init__(self, params: NBeatsParams):
        assert isinstance(params.name, str)
        super(NBeatsTF, self).__init__(name=params.name)
        self.m_name = params.name
        self.n_channels = params.n_channels
        self.forecast_length = params.forecast_length
        self.backcast_length = params.backcast_length
        self.hidden_layer_units = params.hidden_layer_units
        self.nb_blocks_per_stack = params.nb_blocks_per_stack
        self.share_weights_in_stack = params.share_weights_in_stack
        self.stack_types = params.stack_types
        self.stacks = []
        self.thetas_dim = params.thetas_dim
        print(f'| {self.m_name}')
        for stack_id in range(len(self.stack_types)):
            self.stacks.append(self.create_stack(stack_id))

    def create_stack(self, stack_id: int):
        stack_type = self.stack_types[stack_id]
        print(f'| --  Stack {stack_type.title()} (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')
        blocks = []
        for block_id in range(self.nb_blocks_per_stack):
            block_init = NBeatsTF.select_block(stack_type)
            if self.share_weights_in_stack and block_id != 0:
                block = blocks[-1]  # pick up the last one when we share weights.
            else:
                block = block_init(stack_type, stack_id, block_id, self.hidden_layer_units[stack_id],
                                   self.thetas_dim[stack_id], self.backcast_length, self.forecast_length,
                                   self.share_weights_in_stack)
            print(f'     | -- {block}')
            blocks.append(block)
        return blocks

    def call(self, backcast: tf.Tensor, training: Optional[bool] = None,
             mask: Optional[tf.Tensor] = None, stack_out: Optional[Dict] = None):
        """
            backcast: input
            training: training flag
            mask: input mask
            stack_out: stack output collector, type: defaultdict(lambda: 0)
        """
        assert len(backcast.shape) == 3
        assert backcast.shape[-1] == self.n_channels

        backcast = [tf.identity(backcast[:, :, d]) for d in
                    range(self.n_channels)]  # Split the input along the channels
        forecast = [None] * self.n_channels  # Outputs for each channel
        for c in range(self.n_channels):
            for stack_id, stack in enumerate(self.stacks):
                for block_id, block in enumerate(stack):
                    b, f = block(backcast[c], training=training)
                    if stack_out is not None:
                        stack_out[(stack_id, self.stack_types[stack_id])] += tf.identity(f).numpy()
                    forecast[c] = forecast[c] + f if forecast[c] is not None else f
                    backcast[c] = backcast[c] - b

        forecast = tf.stack(forecast, axis=2)
        backcast = tf.stack(backcast, axis=2)

        # Ground out the last backcast as it goes nowhere (dangling tensor, refer to diagram in paper)
        # => doesn't generate a gradient. Needed to clean up gradient compute. By subtracting it from itself
        # gradient generated by the "dangling" tensor is zero (no effect on loss), and at the same time
        # we don't get an undefined gradient compute.

        return forecast + tf.reduce_sum(backcast - backcast, axis=1, keepdims=True)

    def train(self, train_data_fn: Callable, epochs: int, batch_size: int,
              save_model: bool = False, mdl_file_name: str = None) -> float:
        best_val_loss = np.inf
        for epoch in range(epochs):
            print(f'=> Epoch: {epoch + 1}/{epochs}')
            train, val = train_data_fn(gen_validation=True)
            train = train.shuffle(len(train)).batch(batch_size)
            val = val.batch(batch_size)
            logs = self.fit(x=train, epochs=1, validation_data=val)
            if logs.history['val_loss'][0] < best_val_loss:
                best_val_loss = logs.history['val_loss'][0]
                if save_model:
                    self.save(mdl_file_name)
        return best_val_loss

    def save(self, file_name: str):
        self.save_weights(file_name, save_format='tf')

    def load(self, dir: str):
        '''
            dir: directory of the saved model
        '''
        x = np.zeros((1, self.backcast_length, self.n_channels))
        super(NBeatsTF, self).predict(x)
        super(NBeatsTF, self).load_weights(dir)

    @staticmethod
    def select_block(block_type: BlockType):
        if block_type == BlockType.SEASONALITY_BLOCK:
            return SeasonalityBlock
        elif block_type == BlockType.TREND_BLOCK:
            return TrendBlock
        else:
            return GenericBlock

    @staticmethod
    def create_model(netparams: NBeatsParams, trainparms: TrainParams,
                     metric: str = 'smape', run_eagerly: bool = False) -> keras.Model:
        model = NBeatsTF(netparams)
        model.compile(trainparms.optimizer, trainparms.loss,
                      metrics=trainparms.metrics[metric], run_eagerly=run_eagerly)
        return model


def linspace(backcast_length: int, forecast_length: int) -> Tuple[tf.Tensor, tf.Tensor]:
    lin_space = np.arange(-backcast_length, stop=forecast_length, step=1) / backcast_length
    b_ls = lin_space[:backcast_length]
    f_ls = lin_space[backcast_length:]
    return tf.convert_to_tensor(b_ls), tf.convert_to_tensor(f_ls)


def seasonality_model(thetas: tf.Tensor, t: tf.Tensor, H: int) -> tf.Tensor:
    p = thetas.shape[-1]
    assert p <= H, 'thetas_dim too big.'
    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)
    s1 = tf.convert_to_tensor([tf.math.cos(2 * np.pi * i * t) for i in range(p1)], dtype=tf.float32)
    s2 = tf.convert_to_tensor([tf.math.sin(2 * np.pi * i * t) for i in range(p2)], dtype=tf.float32)

    S = tf.concat([s1, s2], axis=0)
    return tf.linalg.matmul(thetas, S)


def trend_model(thetas: tf.Tensor, t: tf.Tensor) -> tf.Tensor:
    p = thetas.shape[-1]
    assert p <= 4, 'thetas_dim is too large.'
    T = tf.convert_to_tensor([t ** i for i in range(p)], dtype=tf.float32)
    return tf.linalg.matmul(thetas, T)


class Block(keras.Model):
    def __init__(self, stack_type: str, stack_id: int, block_id: int, units: int, thetas_dim: int,
                 backcast_length: int, forecast_length: int, share_thetas: bool = False):
        super(Block, self).__init__(name='/'.join([str(stack_id), str(block_id), stack_type]))

        def n(layer_name):
            return '/'.join([str(stack_id), str(block_id), stack_type, layer_name])

        self.units = units
        self.thetas_dim = thetas_dim
        self.backcast_length = backcast_length
        self.forecast_length = forecast_length
        self.share_thetas = share_thetas

        # n*H --> units
        self.fc1 = keras.layers.Dense(units, input_shape=(backcast_length,), activation='relu', name=n('fc1'))
        self.fc2 = keras.layers.Dense(units, activation='relu', name=n('fc2'))
        self.fc3 = keras.layers.Dense(units, activation='relu', name=n('fc3'))
        self.fc4 = keras.layers.Dense(units, activation='relu', name=n('fc3'))

        self.backcast_linspace, self.forecast_linspace = linspace(backcast_length, forecast_length)

        # Thetas: (1), LINEAR, no bias
        if share_thetas:
            self.theta_f_fc = self.theta_b_fc = keras.layers.Dense(thetas_dim, use_bias=False, name=n('theta_bf_fc'))
        else:
            self.theta_b_fc = keras.layers.Dense(thetas_dim, use_bias=False, name=n('theta_b_fc'))
            self.theta_f_fc = keras.layers.Dense(thetas_dim, use_bias=False, name=n('theta_f_fc'))

    def call(self, x: tf.Tensor, training: Optional[bool] = None, mask: Optional[tf.Tensor] = None):
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        x = self.fc4(x)
        return x


class SeasonalityBlock(Block):
    def __init__(self, stack_type: str, stack_id: int, block_id: int, units: int, thetas_dim: int,
                 backcast_length: int, forecast_length: int, share_thetas: bool):
        super(SeasonalityBlock, self).__init__(stack_type, stack_id, block_id, units, thetas_dim,
                                               backcast_length, forecast_length, share_thetas)

    def call(self, x: tf.Tensor, training: Optional[bool] = None,
             mask: Optional[tf.Tensor] = None) -> Tuple[tf.Tensor, tf.Tensor]:
        x = super(SeasonalityBlock, self).call(x, training, mask)
        backcast = seasonality_model(self.theta_b_fc(x), self.backcast_linspace, self.backcast_length)
        forecast = seasonality_model(self.theta_f_fc(x), self.forecast_linspace, self.forecast_length)
        return backcast, forecast


class TrendBlock(Block):
    def __init__(self, stack_type: str, stack_id: int, block_id: int, units: int, thetas_dim: int,
                 backcast_length: int, forecast_length: int, share_thetas: bool):
        # Do not share weights for the trend block
        super(TrendBlock, self).__init__(stack_type, stack_id, block_id, units, thetas_dim,
                                         backcast_length, forecast_length, False)

    def call(self, x: tf.Tensor, training: Optional[bool] = None,
             mask: Optional[tf.Tensor] = None) -> Tuple[tf.Tensor, tf.Tensor]:
        x = super(TrendBlock, self).call(x, training, mask)
        backcast = trend_model(self.theta_b_fc(x), self.backcast_linspace)
        forecast = trend_model(self.theta_f_fc(x), self.forecast_linspace)
        return backcast, forecast


class GenericBlock(Block):
    def __init__(self, stack_type: str, stack_id: int, block_id: int, units: int, thetas_dim: int,
                 backcast_length: int, forecast_length: int, share_thetas: bool = False):
        super(GenericBlock, self).__init__(stack_type, stack_id, block_id, units, thetas_dim, backcast_length,
                                           forecast_length, share_thetas)

        def n(layer_name):
            return '/'.join([str(stack_id), str(block_id), stack_type, layer_name])

        self.backcast_fc = keras.layers.Dense(backcast_length, input_shape=(thetas_dim,),
                                              name=n('backcast_fc'))  # theta_dim --> n*H
        self.forecast_fc = keras.layers.Dense(forecast_length, input_shape=(thetas_dim,),
                                              name=n('forecast_fc'))  # theta_dim --> H

    def call(self, x: tf.Tensor, training: Optional[bool] = None,
             mask: Optional[tf.Tensor] = None) -> Tuple[tf.Tensor, tf.Tensor]:
        x = super(GenericBlock, self).call(x, training, mask)

        theta_b = self.theta_b_fc(x)
        theta_f = self.theta_f_fc(x)

        ''' (GT)
            Basis Vectors (V)

            For the generic block, V are set to the input. As such, the basis "expansion" 
            is simply the weighted mixture of the past signal's portion being analyzed by this
            block (remember, that at the input we're shaving off the component from the past
            that was being analyzed from the previous block. So this block just gets the
            remainder of the signal which it must decide what to combine from it).  
        '''
        backcast = self.backcast_fc(theta_b)  # generic. 3.3., with bias
        forecast = self.forecast_fc(theta_f)  # generic. 3.3., with bias

        return backcast, forecast
